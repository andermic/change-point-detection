\chapter{Methodology}

%TODO:Maybe cite example(s) of non-free-living data
\section{Datasets}
For our experiments we were interested in testing our algorithms on real-world free-living
data. In the past researchers have gathered activity data under unrealistic
laboratory conditions, and unfortunately there are not many labeled free-living
datasets publicly available. We tested our algorithms on two datasets that were
available to us. The first was synthetically generated by concatenating lab
visit data together, while the second was real-world free-living data.

\subsection{Synthetic Data (OSU Hip)}
We generated a synthetic, free-living dataset using physical activity data
collected by Stewart Trost of the Nutrition and Exercise Sciences department of
Oregon State University \cite{trost12} \cite{zheng12}.
This dataset consisted of 91 time series collected over a 2-week period in a
laboratory environment, from 50 children between the ages of 5 and 15. Subjects
performed 12 different types of activities  over two separate lab visits, with
a hip-worn ActiGraph GT1M accelerometer that collected triaxial acceleration
data at a frequency of 30Hz (see top graph of Figure \ref{fig:timeseries}).

Data was collected from two separate visits to the lab, where the subjects performed 6 activities per visit.
Each subject performed the 12 activities in the same order.
In the version of the dataset available to us, we had all 12 activities of
41 of the subjects, only the first 6 activities of an additional 5 subjects,
and the last 6 activities of the remaining 4 subjects.
Subjects were given breaks in between each activity and activities lasted 5-10
minutes, however, these unlabelled breaks were removed from the version that we
used. Additionally, only two minutes of data were available for each subject, so 
time series consisted of six 120 second long activities. We concatenated the
data from these six activities together to create a synthetic, free-living dataset. 
Each of the 91 time series contained a total of $6*120*30 = 21600$ data ticks.

\begin{figure}
 \centering
 \includegraphics[scale=0.27]{osu_timeseries.pdf}
 \includegraphics[scale=0.27]{lime_timeseries.pdf}
 \caption{Sample data from the OSU Hip and LiME datasets. Both datasets are
 triaxial and each axis is shown in a different color.}
 \label{fig:timeseries}
\end{figure}

%\begin{figure}
% \centering
% \includegraphics[scale=0.3]{osu_lying.png}
% \includegraphics[scale=0.3]{osu_writing.png}
% \includegraphics[scale=0.3]{osu_laundry.png}
% \includegraphics[scale=0.3]{osu_catch.png}
% \includegraphics[scale=0.3]{osu_comf_walking.png}
% \includegraphics[scale=0.3]{osu_dancing.png}
% \includegraphics[scale=0.3]{osu_computer.png}
% \includegraphics[scale=0.3]{osu_sweeping.png}
% \includegraphics[scale=0.3]{osu_brisk_walking.png}
% \includegraphics[scale=0.3]{osu_basketball.png}
% \includegraphics[scale=0.3]{osu_running.png}
% \includegraphics[scale=0.3]{osu_treadmill.png}
% \caption{OSU Hip Activity Samples}
% \label{fig:osu_activities}
%\end{figure}

We determined that several of the activities were very similar and that it would
be difficult to discriminate between them. As a result, we combined some of them together to
create a 7-class version of the data.
The classes were lying down, sitting (hand-writing, computer game),
standing (laundry, sweeping, and catch), walking (comfortable, brisk and treadmill walking),
dancing, running, and basketball.

\subsection{LiME}
This dataset consisted of 23 time series, each containing roughly 10 continuous
days worth of data from an individual subject. It was collected by Helen Brown from
the Univerity of Cambridge, and Gemma Ryde from the University of Stirling, Scotland.
Subjects wore an ActiGraph GT3X+
accelerometer during the entire period, which collected triaxial acceleration data at a frequency
of 30Hz, as well as an activPal inclinometer on their thighs. The inclinometer
provided what we considered the ground truth labels of the data by automatically
delimitting and classifying intervals using the orientation of the subject's thigh at any given moment. It 
classified a horizontal orientation as lying down/sitting,
a vertical orientation as standing, and a combination of the two as walking.
The bottom graph of Figure \ref{fig:timeseries} shows a time series segment
from the dataset that contains all 3 activities.

%\begin{figure}
% \centering
% \includegraphics[scale=0.3]{lime_lying.png}
% \includegraphics[scale=0.3]{lime_standing.png}
% \includegraphics[scale=0.3]{lime_walking.png}
% \caption{LiME Day 1 Activity Samples}
% \label{fig:lime_activities}
%\end{figure}

This dataset was challenging to work with because of its size, as each individual time series
contained roughly 25 million ticks of data. To help alleviate this problem, we split each
time series into individual days. We then treated the first full 24 hour period of data that began at midnight,
from each subject, as one whole dataset (LiME Day 1), and the second such period as a separate dataset
(LiME Day 2). We did not use any data from the remaining days.

In contrast to the OSU Hip dataset, LiME
was not synthetic and activity lengths were variable. For LiME Day 1, the
average activity length was 100 seconds with a standard deviation of 881 seconds, and the
median length was 12 seconds. The average number of activities per time series was
871. As would be expected, statistics for LiME Day 2 were comparable: the
average activity length was 104 seconds with a standard deviation of 803 seconds, and the
median length was 12 seconds. The average number of activities per time series was 834.
The medians were relatively small because many of the activities were short,
while the mean and standard deviations were larger because a few of the
activities were extremely long (e.g. when subjects were sleeping).

\section{Featurization}
To formulate our experiments as classification problems, we split each time series into a set of
non-overlapping windows and represented each window as a feature vector.
How we decided where one window ended (and where the next began) varied between
experiments, and is described in sections \ref{sec:topdown} and \ref{sec:bottomup}. Our feature
set was a large collection of statistics that have been shown to be discriminative
for activity classification in previous research \cite{li09} \cite{rothney07}
\cite{staudenmeyer09} \cite{zheng12}. In all we used 18 statistics that were
uniaxial, i.e. were only a function of the data from a single axis of a given window,
as well as one biaxial statistic.
The uniaxial statistics were applied to data from each axis separately, and
the biaxial statistic was applied to data from each of the $C_2^3=3$ possible pairs of
axes, for a total of $18*3+3 = 57$ features.

\vspace{1ex}
\begin{table}[H]
%\small
%\renewcommand{\arraystretch}{1.8}
\centering
\begin{tabular}{|p{15cm}|}  \hline
F1. Sum of values of a period of time: $\sum^T_{i=1} x(i)$.\\ [2ex]
F2. Mean: $\mu_x = \frac{1}{T} \sum^T_{i=1} x(i)$.\\ [2ex]
F3. Standard deviation: $\sigma_x = \sqrt{\frac{1}{T} \sum^T_{i=1} [x(i) - \mu_x]}$.\\ [2ex]
F4. Coefficients of variation: $\frac{\sigma_x}{\mu_x}$. \\ [2ex]
F5. Peak-to-peak amplitude: $max \{x(1), ..., x(T)\} - min \{x(1), .., x(T)\}$.\\ [2ex]
F6-10. Percentiles: $10^{th}, 25^{th}, 50^{th}, 75^{th}, 90^{th}$.\\ [2ex]
F11. Interquartile range: difference between the $75^{th}$ and $25^{th}$ percentiles.\\ [2ex]
F12. Lag-one-autocorrelation: $\frac{\sum^{T-1}_{i=1} [x(i) - \mu_x][x(i+1) - \mu_x]}{\sum^T_{i=1} [x(i) - \mu_x]^2}$.\\ [2ex]
F13. Skewness:
 $\frac{\frac{1}{T} \sum^T_{i=1} [x(i) - \mu_x]^3}
{(\frac{1}{T} \sum^T_{i=1} [x(i) - \mu_x]^2)^\frac{3}{2}}$,
 asymmetry of the signal probability distribution.\\ [2ex]
F14. Kurtosis:
 $\frac{\frac{1}{T} \sum^T_{i=1} [x(i) - \mu_x]^4}
{(\frac{1}{T} \sum^T_{i=1} [x(i) - \mu_x]^2)^3} - 3$,
 peakedness of the signal probability distribution.\\ [2ex]
F15. Signal power: $\sum^T_{i=1} x(i)^2$.\\ [2ex]
F16. Log-energy: $\sum^T_{i=1} \log[x(i)^2]$.\\ [2ex]
F17. Peak intensity: number of signal peak appearances.\\ [2ex]
F18. Zero crossings: number of times the signal crosses its median.\\ [2ex]
F19. Correlation between each pair of axes:
 $\frac{\sum^T_{i=1}[x(i)-\mu_x][v(i)-\mu_v]}
{\sqrt{\sum^T_{i=1}[x(i)-\mu_x] \sum^T_{j=1}[v(j)-\mu_v]}}$.\\ [2ex] \hline
\end{tabular}
\caption{Statistics used to convert time series windows into feature vectors
(taken from \cite{zheng13}). $T$ is the number of ticks in the given time
series, $x$ and $v$ are individual axes of the time series data, and $x(i)$
is the $i$th data tick of $x$.}
\end{table}


%Features (from \cite{zheng13})\\ \hline \\ [-1ex]

One discriminative characteristic of an activity is its overall vigorousness.
The sum [F1] and the sample mean [F2] both act as simple and obvious ways
of measuring vigorousness, as more intense activities will tend to involve
higher rates of acceleration during movement. We also used the 10th [F6], 25th [F7],
50th [F8], 75th [F9], and 90th [F10] percentiles of the data, as well as signal
power [F15] and log energy [F16] as supplemental measures of overall activity intensity.

Another characteristic of an activity is how much it varies in intensity. The sample standard
deviation [F3], coefficient of variation [F4], peak-to-peak amplitude [F5],
number of zero crossings [F18], as well as the
interquartile range [F11] were useful for discriminating between activities
with a consistent level of intensity (low variance, etc.) and activities that were more
rhythmic or staccato in intensity (high variance, etc.). 

Lag-one-autocorrelation [F12], skewness [F13], kurtosis [F14], and peak intensity [F17]
were useful for discriminating between
activities that tend to be similar in their overall intensity and variation in intensity,
but that yielded data with other types of difference in shape. Skewness indicates whether the data is
more concentrated above or below its mean. Kurtosis indicates that the data is concentrated
near its mean or conversely that it is fat-tailed. Lag-one-autocorrelation is a measure of
the general relationship between data ticks and their immediate neighbors in time. Peak
intensity is the number of times that the data repeatedly reached its maximum value. 

Finally we looked at a single bimodal statistic across each pair of axes, the correlation
coefficient [F19], which discriminates between activities where acceleration values in one axis
are predictive of acceleration values in another axis, versus activities where that is not the case.

%TODO: Tie our choice of base classifiers into related work, if possible.
%TODO: Expand this section and/or include a picture.
\section{Base Classifiers}

We tested three classification models on the featurized versions of our data:
decision trees, support vector machines, and neural networks. We used R for our
experiments, and used the R libraries `rpart' \cite{rpart}, `e1071' \cite{svm},
and `nnet' \cite{nnet} to build
our decision tree, SVM, and neural network models, respectively. Default rpart values
were used for the decision tree experiments. The rpart package implements a
procedure for automatically tuning how aggressively it prunes its decision tree
models, and hence our validation set was not used to tune this parameter. 
For the neural net experiments, 
the maximum number of iterations was set to 100000, and the maximum number of
weights was set to 1000000.

For the OSU Hip experiments we tuned the
cost parameter $C$ of the svm on the validation set with 6 values:
$\{0.01,0.1,1,10,100,1000\}$. The single-layer feed-forward neural network took
two tuning parameters, and we tuned with each 2-tuple from the set $N \times W$,
where $N = \{1,2, \ldots, 30\}$ was the numbers of nodes in the hidden layer, and 
$W = \{0,0.5,1\}$ was the weight decay parameter.

Since the LiME datasets were an order of magnitude larger, we tuned them
slightly differently because of time constraints. Setting the $C$ parameter to
1000 proved to be very computationally expensive for the SVM model; instead we tuned $C$
from the values $\{0.01,0.1,1,10,100\}$. Running $30*3=90$ tuning experiments
for the neural networks was also prohibitively expensive; instead we tuned using
values from $N \times W = \{5,10,15\} \times \{0,0.5,1\}$.

\input{topdown.tex}
\input{bottomup.tex}

\section{Performance Metrics}
\begin{figure}[h]
 \centering
 \includegraphics[scale=0.45]{performance.pdf}
 \caption{An example time series with 20 ticks of data, from a dataset with
  three classes: A, B, and C. Four true class labels and their associated
  windows are shown above the axis; four predicted class
  labels and their associated windows are shown below the axis.}
 \label{fig:performance}
\end{figure}

To measure the performance of our classification algorithms we used
two metrics. Accuracy is defined as the number of ticks that an algorithm
correctly classifies in a time series, over the total number of ticks
in the time series. Accuracy is computed by counting the number of
correctly predicted ticks for each true window separately, summing the counts,
and dividing by the total number of ticks. An example section of a time series
is shown in Figure \ref{fig:performance}, and the accuracy of the shown
predictions is given as follows:

\[
\text{Accuracy} = \frac{\text{CPT(A}_1) + \text{CPT(B)} + \text{CPT(C)} + \text{CPT(A}_2)}{(\text{Total number of ticks})} =
\]

\[
\frac{3 + 3 + 0 + 4}{20} = 50\%
\]

where CPT(.) is the number of correctly predicted ticks in an interval, and
A$_1$ and A$_2$ are the true class "A" windows.

Since we were also interested in our algorithms'
feasibility for activity classification in real time, we used detection time as
a second metric. Detection time is computed by counting how many ticks are
required for a prediction algorithm to start correctly predicting the class,
after a true window begins. These counts are summed, and then divided by the number of
true activities.

Consider again the time series segment in Figure \ref{fig:performance}.
Over the true window beginning at tick 1 (class A), the algorithm predicts A
immediately, so the detection time for that window is 0. Over the second true
window beginning at tick 4 (class B), the algorithm does not start predicting
B until tick 6 so the detection time for that window is $6-4=2$. Over the third
true window starting at tick 9 (class C),
the algorithm never predicts C, so the detection time for that window is 3, the
full length of the window. Over the fourth true window starting at tick 12 (class A), the
algorithm starts to predict C at tick 14, but does not predict A until tick 16,
so the detection time for that window is $16 - 12 = 4$. Thus the average
detection time over the time series segment is:

\[
\frac{(0 + 2 + 3 + 4) \; \text{ticks}}{4 \; \text{windows}} = \frac{9}{4} \; \text{ticks per window}
\]

In the change-point detection experiments accuracy and detection time were
averaged over 30 random splits of the given dataset into training, validation, and
testing sets. Because the HMM experiments were more computationally
expensive, accuracy and detection time were averaged over 10 random splits of
the given dataset into training (base classifier), validation, training (HMM),
and testing sets.
