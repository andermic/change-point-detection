
\documentclass[onehalf,11pt]{beavtex}
\title{Activity Detection on Free-Living Data Using Change Point Detection}
\author{Michael M. Anderson}
\degree{Master of Science}
\doctype{Thesis}
\department{Electrical Engineering and Computer Science}
\depttype{School}
\depthead{Director}
\major{Computer Science}
\advisor{Weng-Keen Wong}
\submitdate{January 1, 2013}
\commencementyear{2013}

\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{ {figures} }


\abstract{(Abstract text)}


\acknowledgements{(Acknowledgement text)}


\begin{document}
\maketitle
\mainmatter


\chapter{Introduction}
\section{Motivation}
\section{Previous Research}
\section{Classification Models}
\section{Datasets}


%I have done some excellent research \cite{matrix}.
%\begin{figure}[!ht]
%\centering
%\fbox{\huge Box}
%\caption{Go figure.}
%\end{figure}


\chapter{Top-Down Approach}
\section{Change Point Detection}
For this approach, the data was split into non-overlapping segments for
featurization using techniques from the field of change point detection. 
These techniques find application in control theory and a number of other
problem domains. To detect changes in a time series, it is assumed that each
time tick is a draw from some underlying probability distribution,
but that this underlying distribution will occasionally change at certain 
points in time. To figure out when these
changes occur, a score is generated for each time tick, and all time ticks with
scores above a given threshold are flagged as changes. To generate a score at
a time tick, we consider a window of data that immediately preceeds it
(reference data) and it along with a window of data that immediately follows it (test data).
Scores are calculated by using some metric of dissimilarity or distance
between the reference data and the test data.

There are many different modeling assumptions and associated algorithms
for generating change point detection scores,
including Autoregression, Kernel Density Estimation, Singular Spectrum
Analysis, Exponential Weighted Moving Average, CUSUM
(TODO: include more and explain them more).

\section{Methodology}
We were particularly interested in testing the performance of the
Kullback-Leibler Importance Estimation Proceedure (KLIEP).
This non-parametric approach generates scores using the Kullback-Leibler
divergence between the reference data and the test data. However, instead of
estimating the density of the reference distribution and test distribution
separately, and then comparing them with a likelihood ratio, it models and
estimates the likelihood ratio directly with a Gaussian kernel. Cross-validation 
set the kernel width $\sigma$ at each individual time tick.
We tested this algorithm using a module that was previously implemented in Matlab.
Our reference windows were fixed at a length of 10 seconds, and our test
windows were fixed at a length of 1 second.

We also tested the simpler Control Chart algorithm as a baseline. This
algorithm assumes that each time tick is a draw from a multivariate normal distribution.
It is assumed that no changes occur in the reference window, and the score of a time
tick is the Mahalonobis distance of the tick from the estimated distribution of its reference data.
The mean vector and covariance matrix of the reference data is estimated using
the sample mean and sample standard deviation along each of the 3 axes. For simplicity
we assumed that the covariance between pairs of axes is 0, so the covariance matrix is
diagonal. Our reference windows were fixed at a length of 10 seconds. 

Threshold values were chosen by considering a number of false positive rates of
the change point detection algorithms. A smaller false positive rate per second
corresponded to a higher and more conservative threshold, which split the
time series into fewer segments for featurization. A larger false positive rate
corresponded to a lower threshold, which split the time series into more segments.
The false positive rates that we tested ranged from 0.005 per second to 0.5 per second.

\section{Results}
To measure the performance of a time series classification algorithm we used
two metrics. Accuracy is defined as the number of
ticks that an algorithm correctly classifies in a time series, over the total number of ticks
in the time series. In many applications involving streaming accelerometer data
it is important to quickly detect changes in activity type in real time, so we
were also interested in our algorithms'' detection time.
Detection time is defined as the average number of ticks
required for the algorithm to begin correctly classifying data after a
true activity change has occurred.

As shown in Figure 2.1, we plotted accuracy and detection time as a function of the allowed
false positive rate per second, along with plots of accuracy against detection
time, for the SVM, Decision Tree, and Neural Net classifiers.

\begin{figure}
 \centering
 \includegraphics[scale=0.3]{svm_fps_acc.png}
 \includegraphics[scale=0.3]{svm_fps_det.png}
 \includegraphics[scale=0.3]{svm_det_acc.png}
 \includegraphics[scale=0.3]{dt_fps_acc.png}
 \includegraphics[scale=0.3]{dt_fps_det.png}
 \includegraphics[scale=0.3]{dt_det_acc.png}
 \includegraphics[scale=0.3]{nnet_fpr_acc.png}
 \includegraphics[scale=0.3]{nnet_fpr_det.png}
 \includegraphics[scale=0.3]{nnet_acc_det.png}
 \caption{CPD Classification Performance}
\end{figure}


\chapter{Bottom-Up Approach}
\section{Methodology}
\section{Results}


\chapter{Conclusion}
\section{Discussion}
\section{Directions for Future Research}


\bibliographystyle{plain}
\bibliography{thesis}


%\appendix
%\chapter{Redundancy}
%This appendix is inoperable.
%
\end{document}
